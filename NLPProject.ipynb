{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_e2SPNK94rZ"
      },
      "source": [
        "<h1> <center>GoEmotions   </center>"
      ],
      "id": "t_e2SPNK94rZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhnK77xp94nj"
      },
      "source": [
        "GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.\n",
        "\n",
        "\n",
        "*   Number of examples: 58,009.\n",
        "*   Number of labels: 27 + Neutral.\n",
        "*   Maximum sequence length in training and evaluation datasets: 30.\n",
        "\n",
        "\n",
        "\n",
        "The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise."
      ],
      "id": "DhnK77xp94nj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aQiwtCD94XU"
      },
      "source": [
        "This dataset includes the following columns:\n",
        "\n",
        "*   id: The unique id of the comment.\n",
        "*   text: The text of the comment (with masked tokens, as described in the paper).\n",
        "*   author: The Reddit username of the comment's author.\n",
        "*   example_very_unclear: Whether the annotator marked the example as being very unclear or difficult to label (in this case they did not choose any emotion labels).\n",
        "*   28 other columns for the emotions: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "0aQiwtCD94XU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51872509"
      },
      "outputs": [],
      "source": [
        "#some important imports\n",
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n"
      ],
      "id": "51872509"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCBkLf2mqQrF"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive/')"
      ],
      "id": "iCBkLf2mqQrF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "id": "743b23ad",
        "outputId": "048ebd3f-23d3-48e5-ff02-43d1d7aae709"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-364af7c2-94b9-4f1d-9c31-1306d266cdcb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>example_very_unclear</th>\n",
              "      <th>admiration</th>\n",
              "      <th>amusement</th>\n",
              "      <th>anger</th>\n",
              "      <th>annoyance</th>\n",
              "      <th>approval</th>\n",
              "      <th>caring</th>\n",
              "      <th>confusion</th>\n",
              "      <th>...</th>\n",
              "      <th>love</th>\n",
              "      <th>nervousness</th>\n",
              "      <th>optimism</th>\n",
              "      <th>pride</th>\n",
              "      <th>realization</th>\n",
              "      <th>relief</th>\n",
              "      <th>remorse</th>\n",
              "      <th>sadness</th>\n",
              "      <th>surprise</th>\n",
              "      <th>neutral</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>eew5j0j</td>\n",
              "      <td>That game hurt.</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>eemcysk</td>\n",
              "      <td>&gt;sexuality shouldn’t be a grouping category I...</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ed2mah1</td>\n",
              "      <td>You do right, if you don't care then fuck 'em!</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>eeibobj</td>\n",
              "      <td>Man I love reddit.</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>eda6yn6</td>\n",
              "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-364af7c2-94b9-4f1d-9c31-1306d266cdcb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-364af7c2-94b9-4f1d-9c31-1306d266cdcb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-364af7c2-94b9-4f1d-9c31-1306d266cdcb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        id                                               text  \\\n",
              "0  eew5j0j                                    That game hurt.   \n",
              "1  eemcysk   >sexuality shouldn’t be a grouping category I...   \n",
              "2  ed2mah1     You do right, if you don't care then fuck 'em!   \n",
              "3  eeibobj                                 Man I love reddit.   \n",
              "4  eda6yn6  [NAME] was nowhere near them, he was by the Fa...   \n",
              "\n",
              "   example_very_unclear  admiration  amusement  anger  annoyance  approval  \\\n",
              "0                 False           0          0      0          0         0   \n",
              "1                  True           0          0      0          0         0   \n",
              "2                 False           0          0      0          0         0   \n",
              "3                 False           0          0      0          0         0   \n",
              "4                 False           0          0      0          0         0   \n",
              "\n",
              "   caring  confusion  ...  love  nervousness  optimism  pride  realization  \\\n",
              "0       0          0  ...     0            0         0      0            0   \n",
              "1       0          0  ...     0            0         0      0            0   \n",
              "2       0          0  ...     0            0         0      0            0   \n",
              "3       0          0  ...     1            0         0      0            0   \n",
              "4       0          0  ...     0            0         0      0            0   \n",
              "\n",
              "   relief  remorse  sadness  surprise  neutral  \n",
              "0       0        0        1         0        0  \n",
              "1       0        0        0         0        0  \n",
              "2       0        0        0         0        1  \n",
              "3       0        0        0         0        0  \n",
              "4       0        0        0         0        1  \n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#reading the goEmotions dataset\n",
        "df = pd.read_csv('/content/go_emotions_dataset.csv')\n",
        "df.head()"
      ],
      "id": "743b23ad"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTMUC1ZNa6_H"
      },
      "outputs": [],
      "source": [
        "dfCopy = df.copy()"
      ],
      "id": "KTMUC1ZNa6_H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25266a08",
        "outputId": "51bb70c9-d1cd-422e-cff8-49d5b1c013c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(81496, 31)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#the shape of the dataset\n",
        "df.shape"
      ],
      "id": "25266a08"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9db79bda",
        "outputId": "ed700663-1f46-46d5-d18c-151528825dc5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "id                       object\n",
              "text                     object\n",
              "example_very_unclear     object\n",
              "admiration              float64\n",
              "amusement               float64\n",
              "anger                   float64\n",
              "annoyance               float64\n",
              "approval                float64\n",
              "caring                  float64\n",
              "confusion               float64\n",
              "curiosity               float64\n",
              "desire                  float64\n",
              "disappointment          float64\n",
              "disapproval             float64\n",
              "disgust                 float64\n",
              "embarrassment           float64\n",
              "excitement              float64\n",
              "fear                    float64\n",
              "gratitude               float64\n",
              "grief                   float64\n",
              "joy                     float64\n",
              "love                    float64\n",
              "nervousness             float64\n",
              "optimism                float64\n",
              "pride                   float64\n",
              "realization             float64\n",
              "relief                  float64\n",
              "remorse                 float64\n",
              "sadness                 float64\n",
              "surprise                float64\n",
              "neutral                 float64\n",
              "dtype: object"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#data types of the columns\n",
        "df.dtypes"
      ],
      "id": "9db79bda"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQlZaJaxUtaX",
        "outputId": "f93d8fb8-90eb-4b62-e6eb-bd96f06babe2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "id                      0\n",
              "text                    0\n",
              "example_very_unclear    1\n",
              "admiration              1\n",
              "amusement               1\n",
              "anger                   1\n",
              "annoyance               1\n",
              "approval                1\n",
              "caring                  1\n",
              "confusion               1\n",
              "curiosity               1\n",
              "desire                  1\n",
              "disappointment          1\n",
              "disapproval             1\n",
              "disgust                 1\n",
              "embarrassment           1\n",
              "excitement              1\n",
              "fear                    1\n",
              "gratitude               1\n",
              "grief                   1\n",
              "joy                     1\n",
              "love                    1\n",
              "nervousness             1\n",
              "optimism                1\n",
              "pride                   1\n",
              "realization             1\n",
              "relief                  1\n",
              "remorse                 1\n",
              "sadness                 1\n",
              "surprise                1\n",
              "neutral                 1\n",
              "dtype: int64"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checking for null values\n",
        "df.isna().sum()"
      ],
      "id": "aQlZaJaxUtaX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4zHPFJPEW5b"
      },
      "source": [
        "# <center>Preprocessing</center>"
      ],
      "id": "n4zHPFJPEW5b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9047eda"
      },
      "source": [
        "## Lowercase"
      ],
      "id": "d9047eda"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bc52a51",
        "outputId": "f5cbe93c-fe88-4ed8-be5a-5b0269d10bb4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0                                          that game hurt.\n",
              "1         >sexuality shouldn’t be a grouping category i...\n",
              "2           you do right, if you don't care then fuck 'em!\n",
              "3                                       man i love reddit.\n",
              "4        [name] was nowhere near them, he was by the fa...\n",
              "                               ...                        \n",
              "81491    weird how they shoehorned s character from an ...\n",
              "81492    define woman please if you're not going to use...\n",
              "81493          it was a good sub before the porn took over\n",
              "81494    wait, i see the problem, you are changing the ...\n",
              "81495      that’s crazy how much [name] dunks compared to \n",
              "Name: text, Length: 81496, dtype: object"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#changing the text column to lowercase\n",
        "df[\"text\"] = df[\"text\"].str.lower()\n",
        "df.text"
      ],
      "id": "3bc52a51"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be065432"
      },
      "source": [
        "## Punctuation removal"
      ],
      "id": "be065432"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "08c68305",
        "outputId": "299e70e9-3a61-4d7c-b23b-0b5eb1b49e5e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#All the charcters in the string lib\n",
        "string.punctuation"
      ],
      "id": "08c68305"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e87bd5f9"
      },
      "outputs": [],
      "source": [
        "#Takes a text string as input and removes all punctuation marks from it using the translate method and a string of punctuation characters.\n",
        "def Punc_remove(text):\n",
        "    punc = string.punctuation + \"’‘“”\"\n",
        "    return text.translate(str.maketrans('', '', punc))"
      ],
      "id": "e87bd5f9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5f62332"
      },
      "outputs": [],
      "source": [
        "#This code applies the Punc_remove() function to the 'text' column to remove any text punctuations\n",
        "df['text'] = df['text'].apply(lambda x: Punc_remove(x))"
      ],
      "id": "b5f62332"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb480d01",
        "outputId": "02502190-41b5-4fab-8f22-d3d937069619",
        "scrolled": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0                                           that game hurt\n",
              "1         sexuality shouldnt be a grouping category it ...\n",
              "2               you do right if you dont care then fuck em\n",
              "3                                        man i love reddit\n",
              "4         name was nowhere near them he was by the falcon \n",
              "                               ...                        \n",
              "81491    weird how they shoehorned s character from an ...\n",
              "81492    define woman please if youre not going to use ...\n",
              "81493          it was a good sub before the porn took over\n",
              "81494    wait i see the problem you are changing the wo...\n",
              "81495         thats crazy how much name dunks compared to \n",
              "Name: text, Length: 81496, dtype: object"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['text']"
      ],
      "id": "fb480d01"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c52048da"
      },
      "source": [
        "## Stopping words removal"
      ],
      "id": "c52048da"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d764484b",
        "outputId": "b351a926-55ab-45f9-a982-f784a059eee2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ],
      "id": "d764484b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "497d5823",
        "outputId": "8a4a01de-f0ef-4647-f039-c96f3ebe95f0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"i , me , my , myself , we , our , ours , ourselves , you , you're , you've , you'll , you'd , your , yours , yourself , yourselves , he , him , his , himself , she , she's , her , hers , herself , it , it's , its , itself , they , them , their , theirs , themselves , what , which , who , whom , this , that , that'll , these , those , am , is , are , was , were , be , been , being , have , has , had , having , do , does , did , doing , a , an , the , and , but , if , or , because , as , until , while , of , at , by , for , with , about , against , between , into , through , during , before , after , above , below , to , from , up , down , in , out , on , off , over , under , again , further , then , once , here , there , when , where , why , how , all , any , both , each , few , more , most , other , some , such , no , nor , not , only , own , same , so , than , too , very , s , t , can , will , just , don , don't , should , should've , now , d , ll , m , o , re , ve , y , ain , aren , aren't , couldn , couldn't , didn , didn't , doesn , doesn't , hadn , hadn't , hasn , hasn't , haven , haven't , isn , isn't , ma , mightn , mightn't , mustn , mustn't , needn , needn't , shan , shan't , shouldn , shouldn't , wasn , wasn't , weren , weren't , won , won't , wouldn , wouldn't\""
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#English stopwords separated by commas using the NLTK\n",
        "\" , \".join(stopwords.words('english'))"
      ],
      "id": "497d5823"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd31ab49"
      },
      "outputs": [],
      "source": [
        "#This function removes the stop words from a given text.\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def StopWords_removal(text):\n",
        "    SW = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return SW"
      ],
      "id": "dd31ab49"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed04abc5"
      },
      "outputs": [],
      "source": [
        "#This code applies the StopWords_removal() function to the 'text' column to remove any text stop words\n",
        "df['text'] = df['text'].apply(lambda x: StopWords_removal(x))"
      ],
      "id": "ed04abc5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46964d4f",
        "outputId": "cb0dd2df-5992-4ada-c7c1-2669d5675c0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0                                                game hurt\n",
              "1        sexuality shouldnt grouping category makes dif...\n",
              "2                                  right dont care fuck em\n",
              "3                                          man love reddit\n",
              "4                                 name nowhere near falcon\n",
              "                               ...                        \n",
              "81491    weird shoehorned character ongoing tv show cel...\n",
              "81492    define woman please youre going use accepted d...\n",
              "81493                                   good sub porn took\n",
              "81494    wait see problem changing word observation ass...\n",
              "81495                 thats crazy much name dunks compared\n",
              "Name: text, Length: 81496, dtype: object"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['text']"
      ],
      "id": "46964d4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20bdabb8"
      },
      "source": [
        "## Frequent words"
      ],
      "id": "20bdabb8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70662160",
        "outputId": "e68f617f-70ee-47e2-8e23-16ddd580e3b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('name', 14714),\n",
              " ('like', 6350),\n",
              " ('im', 5880),\n",
              " ('dont', 4561),\n",
              " ('get', 3519),\n",
              " ('thats', 3438),\n",
              " ('one', 3394),\n",
              " ('would', 3268),\n",
              " ('people', 3262),\n",
              " ('love', 3205)]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Counts the frequency of each word in the \"text\" column we didn't remove them as they contain emotions that could help in the classification.\n",
        "from collections import Counter\n",
        "cnt = Counter()\n",
        "for text in df[\"text\"].values:\n",
        "    for word in text.split():\n",
        "        cnt[word] += 1\n",
        "\n",
        "cnt.most_common(10)"
      ],
      "id": "70662160"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJkBF76jXR2I"
      },
      "source": [
        "## Spelling Correction"
      ],
      "id": "LJkBF76jXR2I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-KfVZzweJNZ"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "from tqdm import tqdm"
      ],
      "id": "c-KfVZzweJNZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-QmvjQJnmPu"
      },
      "outputs": [],
      "source": [
        "#word = df.text\n",
        "\n",
        "#result = df['text'].spellcheck()"
      ],
      "id": "R-QmvjQJnmPu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtzxnhUMoHCN"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "# Remove url in the input text\n",
        "def remove_url(input_text: str) -> str:\n",
        "    return re.sub('(www|http)\\S+', '', input_text)"
      ],
      "id": "YtzxnhUMoHCN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrtMI9vqBoj-",
        "outputId": "09099a0c-8488-4bf1-cf6a-1aa8bf94d2db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0                                                game hurt\n",
              "1        sexuality shouldnt grouping category makes dif...\n",
              "2                                  right dont care fuck em\n",
              "3                                          man love reddit\n",
              "4                                 name nowhere near falcon\n",
              "                               ...                        \n",
              "81491    weird shoehorned character ongoing tv show cel...\n",
              "81492    define woman please youre going use accepted d...\n",
              "81493                                   good sub porn took\n",
              "81494    wait see problem changing word observation ass...\n",
              "81495                 thats crazy much name dunks compared\n",
              "Name: text, Length: 81496, dtype: object"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['text'].apply(remove_url)"
      ],
      "id": "PrtMI9vqBoj-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6BgpJ1yq8av"
      },
      "outputs": [],
      "source": [
        " # Remove email in the text\n",
        "def remove_email(input_text: str) -> str:\n",
        "  regex_pattern = '[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}'\n",
        "  return re.sub(regex_pattern, '', input_text)"
      ],
      "id": "_6BgpJ1yq8av"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-ImYPzaBLFd",
        "outputId": "09380844-a4f8-444f-ecc0-9be81ef0872d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0                                                game hurt\n",
              "1        sexuality shouldnt grouping category makes dif...\n",
              "2                                  right dont care fuck em\n",
              "3                                          man love reddit\n",
              "4                                 name nowhere near falcon\n",
              "                               ...                        \n",
              "81491    weird shoehorned character ongoing tv show cel...\n",
              "81492    define woman please youre going use accepted d...\n",
              "81493                                   good sub porn took\n",
              "81494    wait see problem changing word observation ass...\n",
              "81495                 thats crazy much name dunks compared\n",
              "Name: text, Length: 81496, dtype: object"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['text'].apply(remove_email)"
      ],
      "id": "D-ImYPzaBLFd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8uSs8nKr6qO"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional, Union, Callable\n",
        "import os\n",
        "import posixpath as path\n",
        "import ntpath as path\n",
        "_IGNORE_SPELLCHECK_WORD_FILE_PATH = os.path.join('/content/ignore_spellcheck_words.txt')\n",
        "from pathlib import Path\n",
        "\n"
      ],
      "id": "U8uSs8nKr6qO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5Hvj-o7yhvL",
        "outputId": "c560402a-09d1-4c82-aed7-45ad8787be6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspellchecker"
      ],
      "id": "Q5Hvj-o7yhvL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ljxYlOryUpj",
        "outputId": "7d9aab9a-9931-4e2e-c1e6-1348ddd76fab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from spellchecker import SpellChecker\n"
      ],
      "id": "0ljxYlOryUpj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqJRTA6frHK5"
      },
      "outputs": [],
      "source": [
        "#def check_spelling(input_text_or_list: Union[str, List[str]], lang='en',\n",
        "#                   ignore_word_file_path: Union[str, Path] = _IGNORE_SPELLCHECK_WORD_FILE_PATH) -> str:\n",
        "#    \"\"\" Check and correct spellings of the text list \"\"\"\n",
        "#    if input_text_or_list is None or len(input_text_or_list) == 0:\n",
        "#        return ''\n",
        "#    spelling_checker = SpellChecker(language=lang, distance=1)\n",
        "#    # TODO: add acronyms into spell checker to ignore auto correction specified by _IGNORE_SPELLCHECK_WORD_FILE_PATH\n",
        "#    spelling_checker.word_frequency.load_text_file(ignore_word_file_path)\n",
        "#    if isinstance(input_text_or_list, str):\n",
        "#        if not input_text_or_list.islower():\n",
        "#            input_text_or_list = input_text_or_list.lower()\n",
        "#        tokens = word_tokenize(input_text_or_list)\n",
        "#    else:\n",
        "#        tokens = [token.lower() for token in input_text_or_list if token is not None and len(token) > 0]\n",
        "#    misspelled = spelling_checker.unknown(tokens)\n",
        "#    for word in misspelled:\n",
        "#        tokens[tokens.index(word)] = spelling_checker.correction(word)\n",
        "#    return ' '.join(tokens).strip()\n",
        "#    df['text'].apply(check_spelling)\n",
        "#with tqdm(total=len(df)) as pbar:\n",
        "#    for i, row in df.iterrows():\n",
        "#        df.at[i, 'text'] = check_spelling(row['text'])\n",
        "#        pbar.update(1)"
      ],
      "id": "YqJRTA6frHK5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWPpy8UNwFDl"
      },
      "outputs": [],
      "source": [
        "#df['text'].apply(check_spelling)"
      ],
      "id": "rWPpy8UNwFDl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ngpgjc6Ige_x"
      },
      "source": [
        "The code aims to correct the spelling of the text data in a pandas DataFrame column called 'text'. It does this by iterating over each row of the DataFrame, calling the correct_sentence_spelling function on the 'text' column of the row, and replacing the original text with the corrected version.\n",
        "\n",
        "The correct_sentence_spelling function takes a sentence as input, creates a TextBlob object from the sentence, applies spell checking and correction on the sentence using the correct() method of the TextBlob object, and returns the corrected sentence.\n",
        "\n",
        "The iteration over the DataFrame is done using the iterrows() method, which iterates over the rows of the DataFrame as (index, Series) pairs. The at method of the DataFrame is used to update the 'text' column of the current row with the corrected version of the text. Finally, the 'text' column of the DataFrame is updated by applying the correct_sentence_spelling function on each element using the apply() method."
      ],
      "id": "Ngpgjc6Ige_x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5sLcgTzeLyZ"
      },
      "outputs": [],
      "source": [
        "#def correct_sentence_spelling(sentence):\n",
        "#    sentence = TextBlob(sentence)\n",
        "#    result = sentence.correct()\n",
        "#    return result\n",
        "#with tqdm(total=len(df)) as pbar:\n",
        "#    for i, row in df.iterrows():\n",
        "#        df.at[i, 'text'] = correct_sentence_spelling(row['text'])\n",
        "#        pbar.update(1)\n",
        "\n",
        "#df['text'] = df['text'].apply(lambda x: correct_sentence_spelling(x))\n",
        "#df['text']"
      ],
      "id": "F5sLcgTzeLyZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91a037a6"
      },
      "source": [
        "############################################"
      ],
      "id": "91a037a6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "420a5cdb"
      },
      "source": [
        "## Lemmatization"
      ],
      "id": "420a5cdb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "940fd33c",
        "outputId": "81be7edb-220b-4f11-84e8-b27e2eb29aa6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ],
      "id": "940fd33c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VS56IRfZgQD4",
        "outputId": "25473b18-07ae-4466-b309-d765e0db2e67"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "id": "VS56IRfZgQD4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c99c42f"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_words(text):\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])"
      ],
      "id": "0c99c42f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c420e803",
        "outputId": "edcfdf37-92a9-4b3d-af98-65ae65892104"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0                                                game hurt\n",
              "1        sexuality shouldnt grouping category make diff...\n",
              "2                                  right dont care fuck em\n",
              "3                                          man love reddit\n",
              "4                                 name nowhere near falcon\n",
              "                               ...                        \n",
              "81491    weird shoehorned character ongoing tv show cel...\n",
              "81492    define woman please youre going use accepted d...\n",
              "81493                                   good sub porn took\n",
              "81494    wait see problem changing word observation ass...\n",
              "81495                  thats crazy much name dunk compared\n",
              "Name: text, Length: 81496, dtype: object"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"text\"] = df[\"text\"].apply(lambda text: lemmatize_words(text))\n",
        "df.text"
      ],
      "id": "c420e803"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c65ba1d1"
      },
      "outputs": [],
      "source": [
        "#the lemmatization cannot deal only with ing words\n",
        "#lemmatize_words(\"running\")"
      ],
      "id": "c65ba1d1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "242c20b6"
      },
      "source": [
        "## Tokenization"
      ],
      "id": "242c20b6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxWtvRAAWH5w",
        "outputId": "a1d5ad70-1ff1-4eb1-d027-ef779487bf2e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0                                                game hurt\n",
              "1        sexuality shouldnt grouping category make diff...\n",
              "2                                  right dont care fuck em\n",
              "3                                          man love reddit\n",
              "4                                 name nowhere near falcon\n",
              "                               ...                        \n",
              "81491    weird shoehorned character ongoing tv show cel...\n",
              "81492    define woman please youre going use accepted d...\n",
              "81493                                   good sub porn took\n",
              "81494    wait see problem changing word observation ass...\n",
              "81495                  thats crazy much name dunk compared\n",
              "Name: text, Length: 81496, dtype: object"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['text']"
      ],
      "id": "AxWtvRAAWH5w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50c451b2",
        "outputId": "9c85d041-2f1a-4925-d88a-c9504259cfa0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt');"
      ],
      "id": "50c451b2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bdc47a9"
      },
      "outputs": [],
      "source": [
        "def tokenize(column):\n",
        "    tokens = nltk.word_tokenize(column)\n",
        "    return [w for w in tokens if w.isalpha()]"
      ],
      "id": "0bdc47a9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ff6efcc"
      },
      "outputs": [],
      "source": [
        "df['text'] = df['text'].apply(lambda x: tokenize(x))"
      ],
      "id": "5ff6efcc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b6406d3",
        "outputId": "5004748c-141f-4ccd-b4a6-a35e070d9992"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0                                         [game, hurt]\n",
              "1    [sexuality, shouldnt, grouping, category, make...\n",
              "2                        [right, dont, care, fuck, em]\n",
              "3                                  [man, love, reddit]\n",
              "4                        [name, nowhere, near, falcon]\n",
              "Name: text, dtype: object"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['text'].head()"
      ],
      "id": "4b6406d3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHaDgMsQKerl"
      },
      "source": [
        "## Stemming"
      ],
      "id": "KHaDgMsQKerl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hleSAfKT3uLI"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_tokens(tokens):\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return stemmed_tokens\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: stem_tokens(x))"
      ],
      "id": "hleSAfKT3uLI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ1wm1HxHX2W",
        "outputId": "4295e43b-bbea-45ff-c1ac-e6f374ab865a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0                                             [game, hurt]\n",
              "1        [sexual, shouldnt, group, categori, make, diff...\n",
              "2                            [right, dont, care, fuck, em]\n",
              "3                                      [man, love, reddit]\n",
              "4                             [name, nowher, near, falcon]\n",
              "                               ...                        \n",
              "81491    [weird, shoehorn, charact, ongo, tv, show, cel...\n",
              "81492    [defin, woman, pleas, your, go, use, accept, d...\n",
              "81493                              [good, sub, porn, took]\n",
              "81494    [wait, see, problem, chang, word, observ, assu...\n",
              "81495              [that, crazi, much, name, dunk, compar]\n",
              "Name: text, Length: 81496, dtype: object"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['text']"
      ],
      "id": "rJ1wm1HxHX2W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRY8lhEBLlev"
      },
      "outputs": [],
      "source": [],
      "id": "iRY8lhEBLlev"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIrJiOIkHgDz"
      },
      "source": [
        "# <center> word embeddings</center>"
      ],
      "id": "DIrJiOIkHgDz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-bPnWzjHgA6"
      },
      "source": [
        "## CBOW"
      ],
      "id": "A-bPnWzjHgA6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOKuvYmMKWD9",
        "outputId": "8aa5206f-c4c4-46a2-c913-8938dd57d896"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting expects\n",
            "  Downloading expects-0.9.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: expects\n",
            "  Building wheel for expects (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for expects: filename=expects-0.9.0-py3-none-any.whl size=18599 sha256=fa8c0e912c04df62b9d2d4530bfa70e1ace4d4d7be07e8924645db88451caadc\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/e4/dc/7d0873f8e4d68377443095ebf0de4ab6551ffe54fbaa15d580\n",
            "Successfully built expects\n",
            "Installing collected packages: expects\n",
            "Successfully installed expects-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install expects\n"
      ],
      "id": "IOKuvYmMKWD9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzWeO65G4Iul"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "# python\n",
        "import math\n",
        "\n",
        "# from pypi\n",
        "from expects import  be_true\n",
        "from expects import equal\n",
        "from expects import expect\n",
        "import numpy"
      ],
      "id": "XzWeO65G4Iul"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVQiqhbvH5hT"
      },
      "source": [
        "The code is defining a function window_generator which is a generator function that yields windows of words for the continuous bag-of-words (CBOW) model.\n",
        "\n",
        "The function takes two arguments:\n",
        "\n",
        "words: a list of cleaned tokens\n",
        "half_window: an integer that represents the number of words in the half-window. The half-window is the number of words before and after the center word in the window.\n",
        "The function yields a tuple consisting of two parts:\n",
        "\n",
        "context_words: a list of words that are in the context of the center_word. The context words are all the words that are in the half-window.\n",
        "center_word: the word in the center of the window.\n",
        "The generator function iterates over the words list, starting from the half_window index, and ending at len(words) - half_window index. For each index center_index in this range, the function retrieves the center_word from words at index center_index, and creates the context_words list by taking the words from words that appear before and after the center_word. It then yields the tuple consisting of context_words and center_word.\n",
        "\n",
        "Finally, if the generator reaches the end of the loop, it returns None to signal that it is done generating windows"
      ],
      "id": "vVQiqhbvH5hT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dhC7iHY4Q_S"
      },
      "outputs": [],
      "source": [
        "def window_generator(words: list, half_window: int):\n",
        "    \"\"\"Generates windows of words\n",
        "\n",
        "    Args:\n",
        "     words: cleaned tokens\n",
        "     half_window: number of words in the half-window\n",
        "\n",
        "    Yields:\n",
        "     the next window\n",
        "    \"\"\"\n",
        "    for center_index in range(half_window, len(words) - half_window):\n",
        "        center_word = words[center_index]\n",
        "        context_words = (words[(center_index - half_window) : center_index]\n",
        "                         + words[(center_index + 1):(center_index + half_window + 1)])\n",
        "        yield context_words, center_word\n",
        "    return\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "8dhC7iHY4Q_S"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8StgjgHJgIa"
      },
      "source": [
        "This code defines a function called index_word_maps that takes in a list of data as input and returns two dictionaries, word_to_index and index_to_word. The purpose of these dictionaries is to create a mapping between the unique words in the data and their corresponding indices.\n",
        "\n",
        "The function first creates a sorted list of unique words in the data using sorted(list(set(data))). Then, it creates two dictionaries - word_to_index and index_to_word.\n",
        "\n",
        "The word_to_index dictionary maps each unique word to its corresponding index in the sorted list of words. It uses the enumerate() function to loop through the sorted list and assigns an index to each word.\n",
        "\n",
        "The index_to_word dictionary maps each index to its corresponding word in the sorted list of words. It also uses the enumerate() function to loop through the sorted list, but instead of assigning an index to each word, it assigns a word to each index.\n",
        "\n",
        "Finally, the function returns both dictionaries as a tuple."
      ],
      "id": "e8StgjgHJgIa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGyyzue86q_k"
      },
      "outputs": [],
      "source": [
        "def index_word_maps(data: list) -> tuple:\n",
        "    \"\"\"Creates index to word mappings\n",
        "\n",
        "    The index is based on sorted unique tokens in the data\n",
        "\n",
        "    Args:\n",
        "       data: the data you want to pull from\n",
        "\n",
        "    Returns:\n",
        "       word2Ind: returns dictionary mapping the word to its index\n",
        "       Ind2Word: returns dictionary mapping the index to its word\n",
        "    \"\"\"\n",
        "    words = sorted(list(set(data)))\n",
        "\n",
        "    word_to_index = {word: index for index, word in enumerate(words)}\n",
        "    index_to_word = {index: word for index, word in enumerate(words)}\n",
        "    return word_to_index, index_to_word"
      ],
      "id": "vGyyzue86q_k"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXyFQvCSJlTg"
      },
      "source": [
        "This function takes a word, a dictionary that maps words to indices, and the size of the vocabulary as inputs. It creates a one-hot-encoded vector for the given word using numpy.\n",
        "\n",
        "First, it creates a numpy array of zeros with a length equal to the vocabulary size. Then it sets the element at the index corresponding to the given word's index in the dictionary to 1, indicating the presence of the word in the vocabulary.\n",
        "\n",
        "Finally, it returns the one-hot-encoded vector."
      ],
      "id": "aXyFQvCSJlTg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOn-A38u4LHD"
      },
      "outputs": [],
      "source": [
        "def word_to_one_hot_vector(word: str, word_to_index: dict, vocabulary_size: int) -> numpy.ndarray:\n",
        "    \"\"\"Create a one-hot-encoded vector\n",
        "\n",
        "    Args:\n",
        "     word: the word from the corpus that we're encoding\n",
        "     word_to_index: map of the word to the index\n",
        "     vocabulary_size: the size of the vocabulary\n",
        "\n",
        "    Returns:\n",
        "     vector with all zeros except where the word is\n",
        "    \"\"\"\n",
        "    one_hot_vector = numpy.zeros(vocabulary_size)\n",
        "    one_hot_vector[word_to_index[word]] = 1\n",
        "    return one_hot_vector"
      ],
      "id": "xOn-A38u4LHD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIWiB7_0JqtO"
      },
      "source": [
        "This code defines a function context_words_to_vector() which takes two arguments - context_words and word_to_index. context_words is a list of words (strings) that represent the context words around a target word in a sentence, and word_to_index is a dictionary that maps each unique word in the corpus to an integer index.\n",
        "\n",
        "The function converts each word in context_words to a one-hot-encoded vector using the word_to_one_hot_vector() function. It then calculates the mean of all the one-hot vectors along the ROWS axis (which is defined as 0). This results in a vector that represents the average of all the one-hot vectors, which can be thought of as a vector representation of the context words.\n",
        "\n",
        "The function returns this context vector."
      ],
      "id": "nIWiB7_0JqtO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOu4NLwN4LKz"
      },
      "outputs": [],
      "source": [
        "ROWS = 0\n",
        "def context_words_to_vector(context_words: list,\n",
        "                            word_to_index: dict) -> numpy.ndarray:\n",
        "    \"\"\"Create vector with the mean of the one-hot-vectors\n",
        "\n",
        "    Args:\n",
        "     context_words: words to covert to one-hot vectors\n",
        "     word_to_index: dict mapping word to index\n",
        "    \"\"\"\n",
        "    vocabulary_size = len(word_to_index)\n",
        "    context_words_vectors = [\n",
        "        word_to_one_hot_vector(word, word_to_index, vocabulary_size)\n",
        "        for word in context_words]\n",
        "    return numpy.mean(context_words_vectors, axis=ROWS)"
      ],
      "id": "wOu4NLwN4LKz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wluckkOS1zE"
      },
      "source": [
        "The function training_example_generator is a generator that generates training examples for a continuous bag-of-words (CBOW) model. The function takes in three arguments: words, which is a list of tokens (words) in the corpus; half_window, which is an integer that determines the size of the context window (the number of words to the left and right of the target word to consider as context); and word_to_index, which is a dictionary mapping each word in the vocabulary to a unique index.\n",
        "\n",
        "The function uses the window_generator function to generate context words and center words for each window in the corpus. For each center word, the function generates a training example consisting of a feature vector and a target vector. The feature vector is the mean of the one-hot-encoded vectors of the context words, and the target vector is a one-hot-encoded vector of the center word. The function generates the feature and target vectors using the context_words_to_vector and word_to_one_hot_vector functions, respectively.\n",
        "\n",
        "The function yields each training example as a tuple of two numpy arrays: the feature vector and the target vector. The function continues generating training examples until all windows in the corpus have been processed."
      ],
      "id": "4wluckkOS1zE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDvB-3hk4LNr"
      },
      "outputs": [],
      "source": [
        "def training_example_generator(words: list, half_window: int, word_to_index: dict):\n",
        "    \"\"\"generates training examples\n",
        "\n",
        "    Args:\n",
        "     words: source of words\n",
        "     half_window: half the window size\n",
        "     word_to_index: dict with word to index mapping\n",
        "    \"\"\"\n",
        "    vocabulary_size = len(word_to_index)\n",
        "    for context_words, center_word in window_generator(words, half_window):\n",
        "        yield (context_words_to_vector(context_words, word_to_index),\n",
        "               word_to_one_hot_vector(\n",
        "                   center_word, word_to_index, vocabulary_size))\n",
        "    return"
      ],
      "id": "zDvB-3hk4LNr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6IUr89R63Fc"
      },
      "source": [
        "Activation Functions"
      ],
      "id": "O6IUr89R63Fc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa4ZXUvHJxrw"
      },
      "source": [
        "The code contains two functions relu() and softmax() which are used in the CBOW implementation.\n",
        "\n",
        "relu() takes an input array z and returns the ReLU (Rectified Linear Unit) of that array. ReLU is an activation function that is commonly used in neural networks. It applies the function f(x) = max(0,x) to each element of the input array, meaning that it sets negative values to zero and leaves positive values unchanged. The function first creates a copy of the input array, and then replaces all negative values with zero.\n",
        "\n",
        "softmax() takes an input array z and returns an array of probabilities. Softmax is another activation function that is commonly used in neural networks, especially for classification problems. It maps the input array to a probability distribution, meaning that it ensures that the sum of the output probabilities is equal to 1. The function first calculates the exponential of each element in the input array, then calculates the sum of all exponential values, and finally divides each exponential value by the sum to obtain the corresponding probability."
      ],
      "id": "fa4ZXUvHJxrw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sABFvox4LQi"
      },
      "outputs": [],
      "source": [
        "def relu(z: numpy.ndarray) -> numpy.ndarray:\n",
        "    \"\"\"Get the ReLU for the input array\n",
        "\n",
        "    Args:\n",
        "     z: an array of numbers\n",
        "\n",
        "    Returns:\n",
        "     ReLU of z\n",
        "    \"\"\"\n",
        "    result = z.copy()\n",
        "    result[result < 0] = 0\n",
        "    return result\n",
        "def softmax(z: numpy.ndarray) -> numpy.ndarray:\n",
        "    \"\"\"Calculate Softmax for the input\n",
        "\n",
        "    Args:\n",
        "     v: array of values\n",
        "\n",
        "    Returns:\n",
        "     array of probabilities\n",
        "    \"\"\"\n",
        "    e_z = numpy.exp(z)\n",
        "    sum_e_z = numpy.sum(e_z)\n",
        "    return e_z / sum_e_z"
      ],
      "id": "6sABFvox4LQi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5HBp0oC4LWU"
      },
      "outputs": [],
      "source": [],
      "id": "d5HBp0oC4LWU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrjFIk3a67wj"
      },
      "source": [
        "Word Embeddings: Training the CBOW model\n"
      ],
      "id": "UrjFIk3a67wj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUJttYaX7TvE"
      },
      "source": [
        "Neural Network Initialization\n"
      ],
      "id": "WUJttYaX7TvE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uF_syvN54LZL"
      },
      "outputs": [],
      "source": [
        "# Define the size of the word embedding vectors and save it in the variable 'N'\n",
        "N = 3\n",
        "\n",
        "# Define V. Remember this was the size of the vocabulary in the previous lecture notebooks\n",
        "V = 5"
      ],
      "id": "uF_syvN54LZL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnthSZEW7Xfd"
      },
      "source": [
        "Initialization of the weights and biases"
      ],
      "id": "wnthSZEW7Xfd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4i9rypN67SC",
        "outputId": "542701a0-f864-4526-dfe1-b7c1cb9b7e59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "V (vocabulary size): 5\n",
            "N (embedding size / size of the hidden layer): 3\n",
            "size of W1: (3, 5) (NxV)\n",
            "size of b1: (3, 1) (Nx1)\n",
            "size of W2: (5, 3) (VxN)\n",
            "size of b2: (5, 1) (Vx1)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Define the first matrix of weights\n",
        "W1 = numpy.array([\n",
        "    [ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
        "    [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
        "    [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
        "#Define the second matrix of weights\n",
        "W2 = numpy.array([[-0.22182064, -0.43008631,  0.13310965],\n",
        "                  [ 0.08476603,  0.08123194,  0.1772054 ],\n",
        "                  [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
        "                  [ 0.07055222, -0.02015138,  0.36107434],\n",
        "                  [ 0.33480474, -0.39423389, -0.43959196]])\n",
        "#Define the first vector of biases\n",
        "b1 = numpy.array([[ 0.09688219],\n",
        "                  [ 0.29239497],\n",
        "                  [-0.27364426]])\n",
        "#Define the second vector of biases\n",
        "b2 = numpy.array([[ 0.0352008 ],\n",
        "                  [-0.36393384],\n",
        "                  [-0.12775555],\n",
        "                  [-0.34802326],\n",
        "                  [-0.07017815]])\n",
        "#Check that the dimensions of these matrices are correct.\n",
        "\n",
        "print(f'V (vocabulary size): {V}')\n",
        "print(f'N (embedding size / size of the hidden layer): {N}')\n",
        "\n",
        "print(f'size of W1: {W1.shape} (NxV)')\n",
        "print(f'size of b1: {b1.shape} (Nx1)')\n",
        "print(f'size of W2: {W2.shape} (VxN)')\n",
        "print(f'size of b2: {b2.shape} (Vx1)')\n",
        "\n",
        "expect(W1.shape).to(equal((N, V)))\n",
        "expect(b1.shape).to(equal((N, 1)))\n",
        "expect(W2.shape).to(equal((V, N)))\n",
        "expect(b2.shape).to(equal((V, 1)))"
      ],
      "id": "E4i9rypN67SC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDT9BC5M4Lfc",
        "outputId": "7c392ab2-e477-4b36-82f2-e08a20873fb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.25 0.25 0.   0.5  0.  ]\n"
          ]
        }
      ],
      "source": [
        "#Define the tokenized version of the corpus\n",
        "\n",
        "\n",
        "words = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n",
        "\n",
        "\n",
        "#Get 'word_to_index' and 'Ind2word' dictionaries for the tokenized corpus\n",
        "word_to_index, index_to_word = index_word_maps(words)\n",
        "\n",
        "#The First Training Example\n",
        "training_examples = training_example_generator(words, 2, word_to_index)\n",
        "x_array, y_array = next(training_examples)\n",
        "\n",
        "#In this notebook next is used because you will only be performing one iteration of training. In this week's assignment with the full training over several iterations you'll use regular for loops with the iterator that supplies the training examples.\n",
        "\n",
        "#The vector representing the context words, which will be fed into the neural network, is:\n",
        "\n",
        "print(x_array)"
      ],
      "id": "gDT9BC5M4Lfc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRvqDQTN4Lii",
        "outputId": "3c9a1781-26dc-4eba-ef61-07ab6c65bf4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0. 0. 1. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "#The one-hot vector representing the center word to be predicted is:\n",
        "\n",
        "print(y_array)"
      ],
      "id": "BRvqDQTN4Lii"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYGp-qt04Llb",
        "outputId": "42a94301-266d-426f-89c7-59a30b3c7ea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x:\n",
            "[[0.25]\n",
            " [0.25]\n",
            " [0.  ]\n",
            " [0.5 ]\n",
            " [0.  ]]\n",
            "\n",
            "y:\n",
            "[[0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n"
          ]
        }
      ],
      "source": [
        " # Copy vector\n",
        "x = x_array.copy()\n",
        "\n",
        " # Reshape it\n",
        "x.shape = (V, 1)\n",
        "\n",
        " # Print it\n",
        "print(f'x:\\n{x}\\n')\n",
        "\n",
        " # Copy vector\n",
        "y = y_array.copy()\n",
        "\n",
        " # Reshape it\n",
        "y.shape = (V, 1)\n",
        "\n",
        " # Print it\n",
        "print(f'y:\\n{y}')"
      ],
      "id": "dYGp-qt04Llb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxjmnina4Ln5"
      },
      "outputs": [],
      "source": [],
      "id": "qxjmnina4Ln5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUyOr8H18EZH"
      },
      "source": [
        "The Hidden Layer"
      ],
      "id": "IUyOr8H18EZH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiRP8Xcc4Lqq",
        "outputId": "a4d65226-6461-497c-c22e-898c65e500e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.36483875]\n",
            " [ 0.63710329]\n",
            " [-0.3236647 ]]\n"
          ]
        }
      ],
      "source": [
        "z1 = numpy.dot(W1, x) + b1\n",
        "print(z1)"
      ],
      "id": "IiRP8Xcc4Lqq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjiE3qWd8FSo",
        "outputId": "6dfd2253-71d9-49bb-f320-b5a0589e465c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.36483875]\n",
            " [0.63710329]\n",
            " [0.        ]]\n"
          ]
        }
      ],
      "source": [
        "h = relu(z1)\n",
        "print(h)"
      ],
      "id": "LjiE3qWd8FSo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE_l_Cwm8FVn",
        "outputId": "eecddeb2-91f3-479d-a1dd-36078889cede"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-0.31973737]\n",
            " [-0.28125477]\n",
            " [-0.09838369]\n",
            " [-0.33512159]\n",
            " [-0.19919612]]\n"
          ]
        }
      ],
      "source": [
        "#The Output Layer\n",
        "z2 = numpy.dot(W2, h) + b2\n",
        "print(z2)\n",
        "expected = numpy.array([\n",
        "    [-0.31973737],\n",
        "    [-0.28125477],\n",
        "    [-0.09838369],\n",
        "    [-0.33512159],\n",
        "    [-0.19919612]])\n",
        "expect(numpy.allclose(z2, expected)).to(be_true)"
      ],
      "id": "vE_l_Cwm8FVn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNHzWfDX8FYn",
        "outputId": "bbd60cbe-0d7b-4dbf-cf00-9b888f181d88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.18519074]\n",
            " [0.19245626]\n",
            " [0.23107446]\n",
            " [0.18236353]\n",
            " [0.20891502]]\n"
          ]
        }
      ],
      "source": [
        "y_hat = softmax(z2)\n",
        "print(y_hat)\n",
        "expected = numpy.array([\n",
        "    [0.18519074],\n",
        "    [0.19245626],\n",
        "    [0.23107446],\n",
        "    [0.18236353],\n",
        "    [0.20891502]])\n",
        "expect(numpy.allclose(expected, y_hat)).to(be_true)"
      ],
      "id": "jNHzWfDX8FYn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWkrpsfa8FbX",
        "outputId": "c48eace5-afcf-4653-d39b-f5e3b48f0d83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The predicted word at index 2 is 'happy'.\n"
          ]
        }
      ],
      "source": [
        "prediction = numpy.argmax(y_hat)\n",
        "print(f\"The predicted word at index {prediction} is '{index_to_word[prediction]}'.\")"
      ],
      "id": "QWkrpsfa8FbX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NPmhjcq8FeB",
        "outputId": "b45f32fe-226c-4bec-8f9c-97adbb3769e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.18519074]\n",
            " [0.19245626]\n",
            " [0.23107446]\n",
            " [0.18236353]\n",
            " [0.20891502]]\n"
          ]
        }
      ],
      "source": [
        "#Cross-Entropy Loss\n",
        "print(y_hat)"
      ],
      "id": "3NPmhjcq8FeB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AJYP2_88Fg0",
        "outputId": "6644b952-55c6-4822-c212-cd6bc2cef763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n"
          ]
        }
      ],
      "source": [
        "print(y)\n"
      ],
      "id": "6AJYP2_88Fg0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kDeUT1LJ62Q"
      },
      "source": [
        "The cross_entropy_loss function computes the cross-entropy loss between the predicted output and actual output. The function takes in two parameters:\n",
        "\n",
        "y_predicted: a numpy array representing the predicted output of the model.\n",
        "y_actual: a numpy array representing the actual output (labels) of the training data.\n",
        "The function returns the calculated cross-entropy loss.\n",
        "\n",
        "The function first multiplies the y_actual and the natural log of y_predicted element-wise, then computes the negative sum of the resulting array. This is the standard formula for cross-entropy loss."
      ],
      "id": "_kDeUT1LJ62Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WedBh718Fi_"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(y_predicted: numpy.ndarray,\n",
        "                       y_actual: numpy.ndarray) -> numpy.ndarray:\n",
        "    \"\"\"Calculate cross-entropy loss  for the prediction\n",
        "\n",
        "    Args:\n",
        "     y_predicted: what our model predicted\n",
        "     y_actual: the known labels\n",
        "\n",
        "    Returns:\n",
        "     cross-entropy loss for y_predicted\n",
        "    \"\"\"\n",
        "    loss = -numpy.sum(y_actual * numpy.log(y_predicted))\n",
        "    return loss"
      ],
      "id": "-WedBh718Fi_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1xLJouX4LtR",
        "outputId": "d495bfee-08bb-4a78-a83c-e2e50844b8d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.465\n"
          ]
        }
      ],
      "source": [
        "loss = cross_entropy_loss(y_hat, y)\n",
        "print(f\"{loss:0.3f}\")\n",
        "expected = 1.4650152923611106\n",
        "expect(math.isclose(loss, expected)).to(be_true)"
      ],
      "id": "D1xLJouX4LtR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqWvjZ_n8jEX"
      },
      "source": [
        "Backpropagation"
      ],
      "id": "UqWvjZ_n8jEX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ2JqzFBKIg0"
      },
      "source": [
        "This code computes the gradient of the loss function with respect to the bias term b2 of the output layer. The gradient is computed using the predicted output y_hat and the actual target output y.\n",
        "\n",
        "The grad_b2 variable stores the calculated gradient, which is a 2D numpy array. The expected value of grad_b2 is compared to the computed value using the numpy.allclose function. If the expected value and the computed value are almost equal (i.e., within a small tolerance), then the test passes.\n",
        "\n",
        "Overall, this code is likely part of a unit test for the CBOW implementation, where the goal is to ensure that the gradient calculation is correct."
      ],
      "id": "HJ2JqzFBKIg0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_AIVu4v8jjq",
        "outputId": "ad94abc4-45e6-43f5-fbe0-23c5a77621d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.18519074]\n",
            " [ 0.19245626]\n",
            " [-0.76892554]\n",
            " [ 0.18236353]\n",
            " [ 0.20891502]]\n"
          ]
        }
      ],
      "source": [
        "grad_b2 = y_hat - y\n",
        "print(grad_b2)\n",
        "expected = numpy.array([\n",
        "    [ 0.18519074],\n",
        "    [ 0.19245626],\n",
        "    [-0.76892554],\n",
        "    [ 0.18236353],\n",
        "    [ 0.20891502]])\n",
        "expect(numpy.allclose(grad_b2, expected)).to(be_true)"
      ],
      "id": "1_AIVu4v8jjq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjqUU0Hg8d3h",
        "outputId": "683e0f41-bc1c-4655-a523-c732b95be99b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.06756476  0.11798563  0.        ]\n",
            " [ 0.0702155   0.12261452  0.        ]\n",
            " [-0.28053384 -0.48988499  0.        ]\n",
            " [ 0.06653328  0.1161844   0.        ]\n",
            " [ 0.07622029  0.13310045  0.        ]]\n"
          ]
        }
      ],
      "source": [
        "grad_W2 = numpy.dot(y_hat - y, h.T)\n",
        "print(grad_W2)\n",
        "expected = numpy.array([\n",
        "    [0.06756476,  0.11798563,  0.        ],\n",
        "    [ 0.0702155 ,  0.12261452,  0.        ],\n",
        "    [-0.28053384, -0.48988499,  0.        ],\n",
        "    [ 0.06653328,  0.1161844 ,  0.        ],\n",
        "    [ 0.07622029,  0.13310045,  0.        ]])\n",
        "\n",
        "expect(numpy.allclose(grad_W2, expected)).to(be_true)"
      ],
      "id": "PjqUU0Hg8d3h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMpx1x6Q8d40",
        "outputId": "9a371871-9668-47f1-fe9e-ba06a86423bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.        ]\n",
            " [0.        ]\n",
            " [0.17045858]]\n"
          ]
        }
      ],
      "source": [
        "grad_b1 = relu(numpy.dot(W2.T, y_hat - y))\n",
        "print(grad_b1)\n",
        "expected = numpy.array([\n",
        "    [0.        ],\n",
        "    [0.        ],\n",
        "    [0.17045858]])\n",
        "expect(numpy.allclose(grad_b1, expected)).to(be_true)"
      ],
      "id": "VMpx1x6Q8d40"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1gUn4qt8d7d",
        "outputId": "0173c475-a771-4130-9d74-96e3cef8a4d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.04261464 0.04261464 0.         0.08522929 0.        ]]\n"
          ]
        }
      ],
      "source": [
        "grad_W1 = numpy.dot(relu(numpy.dot(W2.T, y_hat - y)), x.T)\n",
        "print(grad_W1)\n",
        "expected = numpy.array([\n",
        "    [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
        "    [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
        "    [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])\n",
        "\n",
        "expect(numpy.allclose(grad_W1, expected)).to(be_true)"
      ],
      "id": "R1gUn4qt8d7d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsMn5l7H8uaQ",
        "outputId": "4bdec45f-cc0b-49c6-b6f0-303d34d04258"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "V (vocabulary size): 5\n",
            "N (embedding size / size of the hidden layer): 3\n",
            "size of grad_W1: (3, 5) (NxV)\n",
            "size of grad_b1: (3, 1) (Nx1)\n",
            "size of grad_W2: (5, 3) (VxN)\n",
            "size of grad_b2: (5, 1) (Vx1)\n"
          ]
        }
      ],
      "source": [
        "print(f'V (vocabulary size): {V}')\n",
        "print(f'N (embedding size / size of the hidden layer): {N}')\n",
        "print(f'size of grad_W1: {grad_W1.shape} (NxV)')\n",
        "print(f'size of grad_b1: {grad_b1.shape} (Nx1)')\n",
        "print(f'size of grad_W2: {grad_W2.shape} (VxN)')\n",
        "print(f'size of grad_b2: {grad_b2.shape} (Vx1)')\n",
        "\n",
        "expect(grad_W1.shape).to(equal((N, V)))\n",
        "expect(grad_b1.shape).to(equal((N, 1)))\n",
        "expect(grad_W2.shape).to(equal((V, N)))\n",
        "expect(grad_b2.shape).to(equal((V, 1)))"
      ],
      "id": "dsMn5l7H8uaQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYHRlasF8z62"
      },
      "source": [
        "Gradient descent\n"
      ],
      "id": "GYHRlasF8z62"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL6SoG0H8uc7",
        "outputId": "077c26ff-d9f4-41f6-9c55-0f2f0d76a2ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "old value of W1:\n",
            "[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n",
            " [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n",
            " [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n",
            "\n",
            "new value of W1:\n",
            "[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n",
            " [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n",
            " [ 0.26509758 -0.2397473  -0.37770863 -0.11655134  0.34008124]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "alpha = 0.03\n",
        "\n",
        "W1_new = W1 - alpha * grad_W1\n",
        "\n",
        "\n",
        "print('old value of W1:')\n",
        "print(W1)\n",
        "print()\n",
        "print('new value of W1:')\n",
        "print(W1_new)"
      ],
      "id": "qL6SoG0H8uc7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uU0WxXL8ugD",
        "outputId": "dd74e24c-5ebd-4765-fe72-dcd9815b0435"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W2_new\n",
            "[[-0.22384758 -0.43362588  0.13310965]\n",
            " [ 0.08265956  0.0775535   0.1772054 ]\n",
            " [ 0.19557112 -0.04637608 -0.1790735 ]\n",
            " [ 0.06855622 -0.02363691  0.36107434]\n",
            " [ 0.33251813 -0.3982269  -0.43959196]]\n",
            "\n",
            "b1_new\n",
            "[[ 0.09688219]\n",
            " [ 0.29239497]\n",
            " [-0.27875802]]\n",
            "\n",
            "b2_new\n",
            "[[ 0.02964508]\n",
            " [-0.36970753]\n",
            " [-0.10468778]\n",
            " [-0.35349417]\n",
            " [-0.0764456 ]]\n"
          ]
        }
      ],
      "source": [
        "W2_new = W2 - alpha * grad_W2\n",
        "\n",
        "\n",
        "b1_new = b1 - alpha * grad_b1\n",
        "\n",
        "\n",
        "b2_new = b2 - alpha * grad_b2\n",
        "print('W2_new')\n",
        "print(W2_new)\n",
        "print()\n",
        "print('b1_new')\n",
        "print(b1_new)\n",
        "print()\n",
        "print('b2_new')\n",
        "print(b2_new)\n",
        "\n",
        "w2_expected = numpy.array(\n",
        "   [[-0.22384758, -0.43362588,  0.13310965],\n",
        "    [ 0.08265956,  0.0775535 ,  0.1772054 ],\n",
        "    [ 0.19557112, -0.04637608, -0.1790735 ],\n",
        "    [ 0.06855622, -0.02363691,  0.36107434],\n",
        "    [ 0.33251813, -0.3982269 , -0.43959196]])\n",
        "\n",
        "b1_expected = numpy.array(\n",
        "   [[ 0.09688219],\n",
        "    [ 0.29239497],\n",
        "    [-0.27875802]])\n",
        "\n",
        "b2_expected = numpy.array(\n",
        "   [[ 0.02964508],\n",
        "    [-0.36970753],\n",
        "    [-0.10468778],\n",
        "    [-0.35349417],\n",
        "    [-0.0764456 ]]\n",
        ")\n",
        "\n",
        "for actual, expected in zip((W2_new, b1_new, b2_new), (w2_expected, b1_expected, b2_expected)):\n",
        "    expect(numpy.allclose(actual, expected)).to(be_true)"
      ],
      "id": "3uU0WxXL8ugD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSNX5CE0F-vR"
      },
      "outputs": [],
      "source": [
        "#import matplotlib.pyplot as plt\n",
        "#import seaborn as sns\n",
        "# Class Balance visualization on GoEmotions\n",
        "#plt.figure(figsize=(20,15))\n",
        "#sns.barplot(x='Percentage', y='Emotion', data=balance_GE, orient='h', hue='Dataset', palette=\"Blues_d\")\n",
        "#plt.title(\"GoEmotions : Percentage of samples per emotion in the train, validation and test datasets\", fontweight='bold')\n",
        "#plt.ylabel(\"Emotions\", fontweight='bold')\n",
        "#plt.xlabel(\"Percentage of all samples\", fontweight='bold')\n",
        "#plt.show()\n",
        "\n"
      ],
      "id": "YSNX5CE0F-vR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPp-qvuBKqF_"
      },
      "source": [
        "## Skip gram"
      ],
      "id": "JPp-qvuBKqF_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8-IN9VWSucM"
      },
      "source": [
        "This section defines three hyperparameters: WINDOW_SIZE, EMBEDDING_SIZE, and LEARNING_RATE. These values can be modified to adjust the behavior of the skip-gram model."
      ],
      "id": "l8-IN9VWSucM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMflyKc5UuUf"
      },
      "outputs": [],
      "source": [
        "# Define constants\n",
        "WINDOW_SIZE = 2\n",
        "EMBEDDING_SIZE = 50\n",
        "LEARNING_RATE = 0.1"
      ],
      "id": "WMflyKc5UuUf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xx7-FhKV_pj"
      },
      "source": [
        "This line creates a list of sentences from a DataFrame column called 'text'."
      ],
      "id": "0xx7-FhKV_pj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehveqzkxU00X"
      },
      "outputs": [],
      "source": [
        "# Define corpus\n",
        "corpus = df['text'].tolist()"
      ],
      "id": "ehveqzkxU00X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk9rrIPpWAsh"
      },
      "source": [
        "This code block creates a dictionary that maps each unique word in the corpus to an index. The defaultdict is a subclass of the built-in dict class that returns a default value when an unknown key is accessed. In this case, the default value is the length of the word_to_idx dictionary, which ensures that each word is assigned a unique index."
      ],
      "id": "zk9rrIPpWAsh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6aDvli2U5YV"
      },
      "outputs": [],
      "source": [
        "# Create word to index mapping\n",
        "word_to_idx = defaultdict(lambda: len(word_to_idx))\n",
        "for sentence in corpus:\n",
        "    for word in sentence:\n",
        "        word_to_idx[word]"
      ],
      "id": "J6aDvli2U5YV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXXbOq7OWLMZ"
      },
      "source": [
        "This line calculates the total number of unique words in the corpus, which is equal to the size of the vocabulary."
      ],
      "id": "FXXbOq7OWLMZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46odT1N4VB5u"
      },
      "outputs": [],
      "source": [
        "# Define vocabulary size\n",
        "vocab_size = len(word_to_idx)\n",
        "# Initialize weight matrices\n",
        "W1 = np.random.randn(vocab_size, EMBEDDING_SIZE)\n",
        "W2 = np.random.randn(EMBEDDING_SIZE, vocab_size)\n"
      ],
      "id": "46odT1N4VB5u"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U92I5xyLWYBq"
      },
      "source": [
        "These lines initialize the weight matrices W1 and W2 with random values drawn from a standard normal distribution. W1 is a matrix that maps each one-hot encoded input word to a dense embedding vector, while W2 is a matrix that maps each embedding vector back to a probability distribution over the entire vocabulary."
      ],
      "id": "U92I5xyLWYBq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86a4MoH4VF_J"
      },
      "outputs": [],
      "source": [
        "# Define softmax function\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n"
      ],
      "id": "86a4MoH4VF_J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7gT-UvUWcTp"
      },
      "source": [
        "This function implements the softmax function, which is used to convert the output of W2 into a probability distribution over the vocabulary. The np.max(x) term is subtracted from x to ensure numerical stability."
      ],
      "id": "f7gT-UvUWcTp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRzyODDUVJJO"
      },
      "outputs": [],
      "source": [
        "# Define one-hot encoding function\n",
        "def one_hot_encoding(word_idx, vocab_size):\n",
        "    x = np.zeros(vocab_size)\n",
        "    x[word_idx] = 1.0\n",
        "    return x\n"
      ],
      "id": "PRzyODDUVJJO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqfMFE1IWkei"
      },
      "source": [
        "This function creates a one-hot encoded vector for a given word index. The vector has a length equal to the size of the vocabulary, and all elements are set to zero except for the element corresponding to the word index, which is set to one."
      ],
      "id": "pqfMFE1IWkei"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lyxHv0qLljD"
      },
      "outputs": [],
      "source": [
        "# Define skip-gram training function\n",
        "def train(corpus, word_to_idx, W1, W2, EMBEDDING_SIZE, WINDOW_SIZE, LEARNING_RATE):\n",
        "    for sentence in corpus:\n",
        "        for i, center_word in enumerate(sentence):\n",
        "            context_words = sentence[max(0, i - WINDOW_SIZE):i] + \\\n",
        "                            sentence[i + 1:min(len(sentence), i + WINDOW_SIZE + 1)]\n",
        "            for context_word in context_words:\n",
        "                center_word_idx = word_to_idx[center_word]\n",
        "                context_word_idx = word_to_idx[context_word]\n",
        "                x = one_hot_encoding(center_word_idx, vocab_size)\n",
        "                y = one_hot_encoding(context_word_idx, vocab_size)\n",
        "                hidden = np.dot(W1.T, x)\n",
        "                output = softmax(np.dot(W2.T, hidden))\n",
        "                error = y - output\n",
        "                dW2 = np.outer(hidden, error)\n",
        "                dW1 = np.outer(x, np.dot(W2, error))\n",
        "                W2 += LEARNING_RATE * dW2\n",
        "                W1 += LEARNING_RATE * dW1\n",
        "    return W1"
      ],
      "id": "0lyxHv0qLljD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySLsWc_lg3uO"
      },
      "source": [
        "The execution wouldn't end more that 5 hours and still nothing\n"
      ],
      "id": "ySLsWc_lg3uO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "9Os_fH7aVPvM",
        "outputId": "fd5834cd-aac3-40eb-b38c-d68910729974"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-176-098d1518a110>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train skip-gram model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mW1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWINDOW_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Extract word embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-174-f098ba2301ec>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(corpus, word_to_idx, W1, W2, EMBEDDING_SIZE, WINDOW_SIZE, LEARNING_RATE)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenter_word_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_word_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train skip-gram model\n",
        "W1 = train(corpus, word_to_idx, W1, W2, EMBEDDING_SIZE, WINDOW_SIZE, LEARNING_RATE)\n",
        "\n",
        "# Extract word embeddings\n",
        "word_embeddings = W1"
      ],
      "id": "9Os_fH7aVPvM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGcysqgVR3JG"
      },
      "source": [
        "## Skip-gram with Negative Sampling"
      ],
      "id": "mGcysqgVR3JG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMwEKBGSg7RD"
      },
      "source": [
        "We then tried the Skip-gram with Negative Sampling so it could run faster"
      ],
      "id": "sMwEKBGSg7RD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QrFDWo2ZPZ0"
      },
      "source": [
        "The code imports necessary packages and sets hyperparameters for the Skip-gram with Negative Sampling model, such as the window size, number of negative samples, learning rate, number of epochs, and the embedding size.\n"
      ],
      "id": "7QrFDWo2ZPZ0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECasSkizZTFR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# set hyperparameters\n",
        "WINDOW_SIZE = 5\n",
        "NUM_NEG_SAMPLES = 5\n",
        "LEARNING_RATE = 0.05\n",
        "NUM_EPOCHS = 5\n",
        "EMBEDDING_SIZE = 100\n"
      ],
      "id": "ECasSkizZTFR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtDWiuqDZWP-"
      },
      "source": [
        "This code creates a vocabulary for the corpus by using the CountVectorizer from scikit-learn to tokenize the text data and extract the vocabulary. X is the sparse matrix representation of the corpus with the count of each vocabulary item in each document, and vocab is a dictionary with the vocabulary items as keys and their corresponding index as values."
      ],
      "id": "rtDWiuqDZWP-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMQtoSh4ZVhe",
        "outputId": "a4f90712-ed94-42d9-a3d1-3ec3b55c811a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# create vocabulary\n",
        "vectorizer = CountVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "vocab = vectorizer.vocabulary_\n"
      ],
      "id": "DMQtoSh4ZVhe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV1ijBKnZgSg"
      },
      "source": [
        "This code creates a co-occurrence matrix for the vocabulary items in the corpus. The co-occurrence matrix is a dictionary where the keys are tuples representing the indices of the co-occurring vocabulary items and the values are the frequency of co-occurrence. The frequency is calculated based on the distance between the two vocabulary items in a document, where closer words have a higher frequency of co-occurrence."
      ],
      "id": "gV1ijBKnZgSg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsPt1gAIZfZP"
      },
      "outputs": [],
      "source": [
        "# create co-occurrence matrix\n",
        "cooc_matrix = defaultdict(float)\n",
        "for row in X.toarray():\n",
        "    indices = np.where(row > 0)[0]\n",
        "    for i in range(len(indices)):\n",
        "        for j in range(max(0, i-WINDOW_SIZE), min(len(indices), i+WINDOW_SIZE)):\n",
        "            if i != j:\n",
        "                cooc_matrix[(indices[i], indices[j])] += 1.0 / np.abs(i-j)"
      ],
      "id": "bsPt1gAIZfZP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I1k5O2zZswv"
      },
      "source": [
        "This code initializes the word embeddings matrix with random values in the range [-0.5/EMBEDDING_SIZE, 0.5/EMBEDDING_SIZE]. The size of the matrix is (vocab_size, EMBEDDING_SIZE), where vocab_size is the number of unique vocabulary items in the corpus and EMBEDDING_SIZE is the dimensionality of the embeddings."
      ],
      "id": "3I1k5O2zZswv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i3OWdhpZtSV"
      },
      "outputs": [],
      "source": [
        "# initialize word embeddings\n",
        "embedding_matrix = (np.random.rand(len(vocab), EMBEDDING_SIZE) - 0.5) / EMBEDDING_SIZE"
      ],
      "id": "_i3OWdhpZtSV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlS1YJN7ZNqs",
        "outputId": "1be50700-738c-4003-ff43-86274a53306e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 Loss: -50163.04428151421\n",
            "Epoch: 2 Loss: -50162.991252269814\n",
            "Epoch: 3 Loss: -50162.892524819654\n",
            "Epoch: 4 Loss: -50162.65577377647\n",
            "Epoch: 5 Loss: -50162.11019805966\n"
          ]
        }
      ],
      "source": [
        "# train word embeddings using Skip-gram with Negative Sampling\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    total_loss = 0.0\n",
        "    for (i, j), cooc in cooc_matrix.items():\n",
        "        # generate negative samples\n",
        "        neg_word_idxs = np.random.choice(len(vocab), NUM_NEG_SAMPLES, replace=False)\n",
        "        context_word_idxs = np.array([i])\n",
        "        word_pair_idxs = np.concatenate((context_word_idxs, neg_word_idxs))\n",
        "\n",
        "        # calculate output and error\n",
        "        u = embedding_matrix[context_word_idxs]\n",
        "        v = embedding_matrix[neg_word_idxs]\n",
        "        z = 1 / (1 + np.exp(-np.dot(u, v.T)))\n",
        "        dL_dz = (1 - cooc / (cooc + NUM_NEG_SAMPLES)) - z\n",
        "        total_loss += (cooc / (cooc + NUM_NEG_SAMPLES)) * np.log(z).sum() + (NUM_NEG_SAMPLES / (NUM_NEG_SAMPLES + cooc)) * np.log(1 - z).sum()\n",
        "\n",
        "        # update word embeddings\n",
        "        dL_du = np.dot(dL_dz, v) / len(word_pair_idxs)\n",
        "        dL_dv = np.dot(dL_dz.T, u) / len(word_pair_idxs)\n",
        "        embedding_matrix[context_word_idxs] -= LEARNING_RATE * dL_du\n",
        "        embedding_matrix[neg_word_idxs] -= LEARNING_RATE * dL_dv\n",
        "\n",
        "    print('Epoch:', epoch+1, 'Loss:', total_loss)\n"
      ],
      "id": "YlS1YJN7ZNqs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUoJvQKOeHM4"
      },
      "outputs": [],
      "source": [],
      "id": "qUoJvQKOeHM4"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}