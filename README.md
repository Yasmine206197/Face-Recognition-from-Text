# Face-Recognition-from-Text---- 
The dataset is the go emotions dataset provided freely by Google which contains 58k Reddit comments.
The dataset was divided into three parts; training set, validation set and testing set. They were found to be free from null values. The tweets were all converted to lowercases thereby facilitating the processing, post to which punctuations were removed as they are prone to induce the models to errors. Stopping words removal was also done prior to checking the most frequent words and applying a spelling correction. The goal of using a spelling corrector was to help speed up the model phases as they will spend less time trying to first correct them before the processing is done.
Tokenization was done prior to label encoding which helps convert these sentences to binary representations that will be made easier for the model to understand. Stemming helped reduce the tokenized words to their respective base forms. For example running will be changed to run. The word2vec was pretrained to help vectorize all the encoded words. The vocabulary table will help our model interpret the emotions behind the texts. 
